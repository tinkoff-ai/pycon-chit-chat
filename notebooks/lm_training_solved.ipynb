{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "\n",
    "–ü–µ—Ä–µ–¥ —Ç–µ–º –∫–∞–∫ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å, –Ω—É–∂–Ω–æ –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–ª—è —ç—Ç–æ–≥–æ –¥–∞–Ω–Ω—ã–µ. \n",
    "\n",
    "–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –Ω–µ–π–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –∏ –≤ –æ—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏ –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤–Ω—ã–µ –º–æ–¥–µ–ª–∏, –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ –æ—á–µ–Ω—å –±–æ–ª—å—à–∏—Ö –¥–∞–Ω–Ω—ã—Ö. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∏—Ö —Å –ø–æ–º–æ—â—å—é –≤—Å–µ–º –∏–∑–≤–µ—Å—Ç–Ω–æ–π –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ `pandas` –∑–∞—Ç—Ä—É–¥–Ω–∏—Ç–µ–ª—å–Ω–∞ –ø–æ —Ä—è–¥—É –ø—Ä–∏—á–∏–Ω (–Ω–µ—Ç –º–Ω–æ–≥–æ–ø—Ç–æ—á–Ω–æ—Å—Ç–∏, –∫–µ—à–∏—Ä–æ–≤–∞–Ω–∏—è, –Ω–µ—É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ö—Ä–∞–Ω–µ–Ω–∏—è, ...)\n",
    "\n",
    "–î–ª—è —Ä–∞–±–æ—Ç—ã —Å –±–æ–ª—å—à–∏–º–∏ –æ–±—ä–µ–º–∞–º–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –±–∏–±–ª–∏–æ—Ç–µ–∫ (`arrow`, `pil`, ...). –í —ç—Ç–æ–º –Ω–æ—É—Ç–±—É–∫–µ –º—ã –±—É–¥–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–∏–±–ª–∏–æ—Ç–µ–∫—É `datasets` –æ—Ç *hugging-face*, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–∞ `arrow`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d.tsimerman/blowjob/pycon-chit-chat/.venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-4b39c19aab5c9a43\n",
      "Reusing dataset csv (/Users/d.tsimerman/.cache/huggingface/datasets/csv/default-4b39c19aab5c9a43/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 35.37it/s]\n"
     ]
    }
   ],
   "source": [
    "data = load_dataset('csv', \n",
    "    data_files={\n",
    "        'train': '../data.csv'\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context_3', 'context_2', 'context_1', 'response'],\n",
       "        num_rows: 2507\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "data = data['train'].train_test_split(test_size=0.2)  # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['context_3', 'context_2', 'context_1', 'response'],\n",
       "        num_rows: 2005\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['context_3', 'context_2', 'context_1', 'response'],\n",
       "        num_rows: 502\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_3': ['–ß–µ–º –∑–∞–ø–æ–º–Ω–∏–ª—Å—è –¥–µ–Ω—å?', None],\n",
       " 'context_2': ['–ö–æ—Ç–∏–∫ –∂–∏–≤–æ—Ç –∫—É—Å–∞–ª))', None],\n",
       " 'context_1': ['–û—Ç–µ –º–∏–ª–æ)',\n",
       "  '–ë—É–¥–µ—Ç –º–∞—Å–∫–∞. –ê —Ç–æ –ø—Ä—ã—â –≤—ã—Å–∫–æ—á–∏–ª .\\n–ì–æ–≤–æ—Ä—è—Ç –ø–æ–º–æ–≥–∞–µ—Ç.'],\n",
       " 'response': [None, '–ú–Ω–µ –∫–æ–∂—É —Å—É—à–∏—Ç']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][0:2]  # First two rows of the dataset of type Dict[FeatureName, List[values]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è —Å –ø–æ–º–æ—â—å—é 2—Ö –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π -- `filter` –∏ `map` . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      " \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwith_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_columns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mload_from_cache_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_file_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfn_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdesc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DatasetDict'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Apply a filter function to all the elements in the table in batches\n",
      "and update the table so that the dataset only includes examples according to the filter function.\n",
      "The transformation is applied to all the datasets of the dataset dictionary.\n",
      "\n",
      "Args:\n",
      "    function (`callable`): with one of the following signature:\n",
      "        - ``function(example: Dict[str, Any]) -> bool`` if ``with_indices=False, batched=False``\n",
      "        - ``function(example: Dict[str, Any], indices: int) -> bool`` if ``with_indices=True, batched=False``\n",
      "        - ``function(example: Dict[str, List]) -> List[bool]`` if ``with_indices=False, batched=True``\n",
      "        - ``function(example: Dict[str, List], indices: List[int]) -> List[bool]`` if ``with_indices=True, batched=True``\n",
      "    with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      "    input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      "        positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      "    batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      "    batch_size (:obj:`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      "        `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      "    keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      "    load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      "        can be identified, use it instead of recomputing.\n",
      "    cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
      "        results of the computation instead of the automatically generated cache file name.\n",
      "        You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      "    writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      "        This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      "        Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      "    fn_kwargs (:obj:`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n",
      "    num_proc (:obj:`int`, optional, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      "        use multiprocessing.\n",
      "    desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
      "\n",
      "Example:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> ds.filter(lambda x: x[\"label\"] == 1)\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 4265\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 533\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label'],\n",
      "        num_rows: 533\n",
      "    })\n",
      "})\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/blowjob/pycon-chit-chat/.venv/lib/python3.8/site-packages/datasets/dataset_dict.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "? data.filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      " \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfunction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwith_indices\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwith_rank\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minput_columns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatched\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdrop_last_batch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mremove_columns\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mload_from_cache_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mcache_file_names\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mwriter_batch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdisable_nullable\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mfn_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mnum_proc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mdesc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m'DatasetDict'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Apply a function to all the elements in the table (individually or in batches)\n",
      "and update the table (if function does updated examples).\n",
      "The transformation is applied to all the datasets of the dataset dictionary.\n",
      "\n",
      "Args:\n",
      "    function (`callable`): with one of the following signature:\n",
      "        - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False`\n",
      "        - `function(example: Dict[str, Any], indices: int) -> Dict[str, Any]` if `batched=False` and `with_indices=True`\n",
      "        - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False`\n",
      "        - `function(batch: Dict[str, List], indices: List[int]) -> Dict[str, List]` if `batched=True` and `with_indices=True`\n",
      "\n",
      "        For advanced usage, the function can also return a `pyarrow.Table`.\n",
      "        Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
      "\n",
      "    with_indices (`bool`, defaults to `False`): Provide example indices to `function`. Note that in this case the signature of `function` should be `def function(example, idx): ...`.\n",
      "    with_rank (:obj:`bool`, default `False`): Provide process rank to `function`. Note that in this case the\n",
      "        signature of `function` should be `def function(example[, idx], rank): ...`.\n",
      "    input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): The columns to be passed into `function` as\n",
      "        positional arguments. If `None`, a dict mapping to all formatted columns is passed as one argument.\n",
      "    batched (`bool`, defaults to `False`): Provide batch of examples to `function`\n",
      "    batch_size (:obj:`int`, optional, defaults to `1000`): Number of examples per batch provided to `function` if `batched=True`\n",
      "        `batch_size <= 0` or `batch_size == None`: Provide the full dataset as a single batch to `function`\n",
      "    drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n",
      "        dropped instead of being processed by the function.\n",
      "    remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`): Remove a selection of columns while doing the mapping.\n",
      "        Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
      "        columns with names in `remove_columns`, these columns will be kept.\n",
      "    keep_in_memory (`bool`, defaults to `False`): Keep the dataset in memory instead of writing it to a cache file.\n",
      "    load_from_cache_file (`bool`, defaults to `True`): If a cache file storing the current computation from `function`\n",
      "        can be identified, use it instead of recomputing.\n",
      "    cache_file_names (`Optional[Dict[str, str]]`, defaults to `None`): Provide the name of a path for the cache file. It is used to store the\n",
      "        results of the computation instead of the automatically generated cache file name.\n",
      "        You have to provide one :obj:`cache_file_name` per dataset in the dataset dictionary.\n",
      "    writer_batch_size (:obj:`int`, default `1000`): Number of rows per write operation for the cache file writer.\n",
      "        This value is a good trade-off between memory usage during the processing, and processing speed.\n",
      "        Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `.map()`.\n",
      "    features (`Optional[datasets.Features]`, defaults to `None`): Use a specific Features to store the cache file\n",
      "        instead of the automatically generated one.\n",
      "    disable_nullable (`bool`, defaults to `False`): Disallow null values in the table.\n",
      "    fn_kwargs (:obj:`Dict`, optional, defaults to `None`): Keyword arguments to be passed to `function`\n",
      "    num_proc (:obj:`int`, optional, defaults to `None`): Number of processes for multiprocessing. By default it doesn't\n",
      "        use multiprocessing.\n",
      "    desc (:obj:`str`, optional, defaults to `None`): Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
      "\n",
      "Example:\n",
      "\n",
      "```py\n",
      ">>> from datasets import load_dataset\n",
      ">>> ds = load_dataset(\"rotten_tomatoes\")\n",
      ">>> def add_prefix(example):\n",
      "...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
      "...     return example\n",
      ">>> ds = ds.map(add_prefix)\n",
      ">>> ds[\"train\"][0:3][\"text\"]\n",
      "['Review: the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .',\n",
      " 'Review: the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .',\n",
      " 'Review: effective but too-tepid biopic']\n",
      "\n",
      "# process a batch of examples\n",
      ">>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
      "# set number of processors\n",
      ">>> ds = ds.map(add_prefix, num_proc=4)\n",
      "```\n",
      "\u001b[0;31mFile:\u001b[0m      ~/blowjob/pycon-chit-chat/.venv/lib/python3.8/site-packages/datasets/dataset_dict.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "? data.map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–û—Å—Ç–∞–≤–∏–º —Ç–æ–ª—å–∫–æ —Ç–µ –ø—Ä–∏–º–µ—Ä—ã, –≥–¥–µ context_1 –Ω–µ –ø—É—Å—Ç–æ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 25.21ba/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 101.73ba/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.filter(lambda sample: sample['context_1'] is not None and sample['response'] is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_3': [None, '–ü—Ä–æ –¥—Ä—É–≥–æ–π –æ—Ä–µ—Ö –∑–∞–≥–æ–≤–æ—Ä–∏–ª–∏üòÇ'],\n",
       " 'context_2': [None, '–ö–∞–∫–æ–π???'],\n",
       " 'context_1': ['–ë—É–¥–µ—Ç –º–∞—Å–∫–∞. –ê —Ç–æ –ø—Ä—ã—â –≤—ã—Å–∫–æ—á–∏–ª .\\n–ì–æ–≤–æ—Ä—è—Ç –ø–æ–º–æ–≥–∞–µ—Ç.',\n",
       "  '–î–∞ –ø—Ä–æ –ø–æ–ø—ÉüòÇ'],\n",
       " 'response': ['–ú–Ω–µ –∫–æ–∂—É —Å—É—à–∏—Ç',\n",
       "  '–Ø –ø–æ—Ö–æ–¥—É –≤ –ª–µ–¥–∏ –ø—Ä–µ–≤—Ä–∞—â–∞—é—Å—å))) —Å–∫—Ä–æ–º–Ω—É—é –∏ –Ω—É–¥–Ω—É—éüòÇüòÇüòÇüòÇ']}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train'][:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü—Ä–∏–≤–æ–¥–∏–º –∫–∞–∂–¥—ã–π –ø—Ä–∏–º–µ—Ä –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫ –≤–∏–¥—É –æ–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "FIRST_SPEAKER_TOKEN = '@@–ü–ï–†–í–´–ô@@'\n",
    "SECOND_SPEAKER_TOKEN = '@@–í–¢–û–†–û–ô@@'\n",
    "\n",
    "CONTEXT_COLS = ['context_3', 'context_2', 'context_1']\n",
    "RESPONSE_COL = ['response']\n",
    "\n",
    "def convert_to_dialog(sample: Dict[str, str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "        Convert sample row to dialogs str format\n",
    "    \"\"\"\n",
    "    text = FIRST_SPEAKER_TOKEN + ' '\n",
    "    for col in CONTEXT_COLS + RESPONSE_COL:\n",
    "        next_special_token = FIRST_SPEAKER_TOKEN if text.strip().endswith(SECOND_SPEAKER_TOKEN) else SECOND_SPEAKER_TOKEN\n",
    "\n",
    "        if (col not in sample) or (not sample[col]):\n",
    "            continue\n",
    "\n",
    "        text += sample[col]\n",
    "        if col != RESPONSE_COL[0]:\n",
    "            text += ' ' + next_special_token + ' '\n",
    "    \n",
    "    return {'text': text}\n",
    "\n",
    "assert convert_to_dialog(\n",
    "    {\n",
    "        'context_3': '–ø—Ä–∏–≤–µ—Ç',\n",
    "        'context_2': '–ø—Ä–∏–≤–µ—Ç!',\n",
    "        'context_1': '–∫–∞–∫ –¥–µ–ª–∞?',\n",
    "        'response': '—Å—É–ø–µ—Ä)'\n",
    "    }\n",
    ") == {'text': '@@–ü–ï–†–í–´–ô@@ –ø—Ä–∏–≤–µ—Ç @@–í–¢–û–†–û–ô@@ –ø—Ä–∏–≤–µ—Ç! @@–ü–ï–†–í–´–ô@@ –∫–∞–∫ –¥–µ–ª–∞? @@–í–¢–û–†–û–ô@@ —Å—É–ø–µ—Ä)'}\n",
    "assert convert_to_dialog(\n",
    "    {\n",
    "        'context_1': '–∫–∞–∫ –¥–µ–ª–∞?',\n",
    "        'response': '—Å—É–ø–µ—Ä)'\n",
    "    }\n",
    ") == {'text': '@@–ü–ï–†–í–´–ô@@ –∫–∞–∫ –¥–µ–ª–∞? @@–í–¢–û–†–û–ô@@ —Å—É–ø–µ—Ä)'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1334/1334 [00:00<00:00, 10816.69ex/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 325/325 [00:00<00:00, 7789.51ex/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(convert_to_dialog, remove_columns=CONTEXT_COLS + RESPONSE_COL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1334\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 325\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–¢–µ–ø–µ—Ä—å –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —Ç–æ–∫–µ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å, —Ç–æ—á–Ω–æ —Ç–∞–∫–∂–µ –∫–∞–∫ –º—ã —ç—Ç–æ –¥–µ–ª–∞–ª–∏ –ø—Ä–∏ –∑–Ω–∞–∫–æ–º—Å—Ç–≤–µ —Å —Ö–∞–≥–≥–∏–Ω—Ñ–µ–π—Å"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def tokenize_sample(sample: Dict[str, str]):\n",
    "    return tokenizer(sample['text'], max_length=512, truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1334/1334 [00:00<00:00, 3365.46ex/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 325/325 [00:00<00:00, 3674.44ex/s]\n"
     ]
    }
   ],
   "source": [
    "data = data.map(tokenize_sample, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 1334\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask'],\n",
       "        num_rows: 325\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –û–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å, –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ–º –æ–±—É—á–∞—Ç—å, –∞ —Ç–∞–∫–∂–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫–ª–∞—Å—Å—ã -- trainer, training_args –∏ datacollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AutoModelForCausalLM.from_pretrained('tinkoff-ai/ruDialoGPT-small').to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–£–∫–∞–∑—ã–≤–∞–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –∏ —Å–æ–∑–¥–∞–µ–º –∫–ª–∞—Å—Å Trainer (https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "arguments = {\n",
    "    'output_dir': './training_output',  # path to save the model's checkpoints\n",
    "    'per_device_train_batch_size': 4,  # batch size per GPU/CPU for training\n",
    "    'gradient_accumulation_steps': 4,  # number of batches to accumulate gradient\n",
    "    'max_steps': 10,  # total number of optimizer.step() calls\n",
    "    'save_steps': 10,  # save every save_steps\n",
    "    'eval_steps': 10,  # run evaluation every eval_steps\n",
    "    'dataloader_num_workers': 0,  # number of workers for data loading (default: 0)\n",
    "    'save_total_limit': 2,  # total number of checkpoints to save, delete older checkpoints when reached\n",
    "}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(**arguments),\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False),\n",
    "    train_dataset=data['train'],\n",
    "    eval_dataset=data['test']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ó–∞–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/d.tsimerman/blowjob/pycon-chit-chat/.venv/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1334\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 10\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:34<00:00,  3.29s/it]Saving model checkpoint to ./training_output/checkpoint-10\n",
      "Configuration saved in ./training_output/checkpoint-10/config.json\n",
      "Model weights saved in ./training_output/checkpoint-10/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:35<00:00,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 35.7758, 'train_samples_per_second': 4.472, 'train_steps_per_second': 0.28, 'train_loss': 4.498283386230469, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10, training_loss=4.498283386230469, metrics={'train_runtime': 35.7758, 'train_samples_per_second': 4.472, 'train_steps_per_second': 0.28, 'train_loss': 4.498283386230469, 'epoch': 0.12})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### –ü—Ä–æ–±—É–µ–º –æ–±—É—á–µ–Ω–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/tinkoff-ai/ruDialoGPT-small/resolve/main/vocab.json from cache at /Users/d.tsimerman/.cache/huggingface/transformers/7b39d42ced5110cdcc7e4bdb46c98242413a9f53c1f19df10b3445e53e563090.c483bc3440d25937fdac74506b73b76ee6e67f778a804756214363fc2a1a66ef\n",
      "loading file https://huggingface.co/tinkoff-ai/ruDialoGPT-small/resolve/main/merges.txt from cache at /Users/d.tsimerman/.cache/huggingface/transformers/d0bf521c0f60cb2c5e95da8868245bd528cd0c1cf829616231b00e0b5bdca3cd.7362c0dbb32f750eeea5a5b93bbd0c6876eac41453369265d5a49df1c9142b6f\n",
      "loading file https://huggingface.co/tinkoff-ai/ruDialoGPT-small/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/tinkoff-ai/ruDialoGPT-small/resolve/main/added_tokens.json from cache at /Users/d.tsimerman/.cache/huggingface/transformers/8969e3b2f8db62ca0eadb45ff1085c9cc2989ac3f7db0e14af9977ba83a0573d.c0d3a65a693915a101abad0f4d9e99b33fc983fd2037e92cfb6788725c50a700\n",
      "loading file https://huggingface.co/tinkoff-ai/ruDialoGPT-small/resolve/main/special_tokens_map.json from cache at /Users/d.tsimerman/.cache/huggingface/transformers/2f436b3c4b25afb10f8e17178f4efa5d5726f348a8938d44e786ae938e1b2a34.a1718bf1888da53a46be06b8f4da9ed4b6077026dd1a3051062f38b7d55052e8\n",
      "loading file https://huggingface.co/tinkoff-ai/ruDialoGPT-small/resolve/main/tokenizer_config.json from cache at /Users/d.tsimerman/.cache/huggingface/transformers/c6e5b8531e52b6b68e512a39c5cebc4380394bc38f72b0cf7036b3d8301f2363.42dc149d4ddbe208d417960875ef2f4f92919e56cdb5881bb0ff10ef5f1b52db\n",
      "Adding @@–ü–ï–†–í–´–ô@@ to the vocabulary\n",
      "Adding @@–í–¢–û–†–û–ô@@ to the vocabulary\n",
      "Adding <FIRST_SPEAKER> to the vocabulary\n",
      "Adding <SECOND_SPEAKER> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "/Users/d.tsimerman/blowjob/pycon-chit-chat/.venv/lib/python3.8/site-packages/transformers/models/auto/modeling_auto.py:998: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n",
      "loading configuration file ./training_output/checkpoint-10/config.json\n",
      "Model config GPT2Config {\n",
      "  \"_name_or_path\": \"./training_output/checkpoint-10/\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 2048,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 2048,\n",
      "  \"reorder_and_upcast_attn\": false,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"scale_attn_by_inverse_layer_idx\": false,\n",
      "  \"scale_attn_weights\": true,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50261\n",
      "}\n",
      "\n",
      "loading weights file ./training_output/checkpoint-10/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing GPT2LMHeadModel.\n",
      "\n",
      "All the weights of GPT2LMHeadModel were initialized from the model checkpoint at ./training_output/checkpoint-10/.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelWithLMHead\n",
    "\n",
    "checkpoint_path = './training_output/checkpoint-10/'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-small')\n",
    "model = AutoModelWithLMHead.from_pretrained(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/d.tsimerman/blowjob/pycon-chit-chat/.venv/lib/python3.8/site-packages/transformers/generation_utils.py:1202: UserWarning: Neither `max_length` nor `max_new_tokens` have been set, `max_length` will default to 20 (`self.config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@@–ü–ï–†–í–´–ô@@ –ø—Ä–∏–≤–µ—Ç @@–í–¢–û–†–û–ô@@ –ø—Ä–∏–≤–µ—Ç @@–ü–ï–†–í–´–ô@@ –∫–∞–∫ —É —Ç–µ–±—è–¥–µ–ª–∞? @@–í–¢–û–†–û–ô@@      ']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer('@@–ü–ï–†–í–´–ô@@ –ø—Ä–∏–≤–µ—Ç @@–í–¢–û–†–û–ô@@ –ø—Ä–∏–≤–µ—Ç @@–ü–ï–†–í–´–ô@@ –∫–∞–∫ —É —Ç–µ–±—è–¥–µ–ª–∞? @@–í–¢–û–†–û–ô@@ ', return_tensors='pt')\n",
    "generated_token_ids = model.generate(**inputs)\n",
    "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
    "context_with_response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3ef03fb1679aeac65da8746bbb9907ec69320f4b4b60f937c1b2a3f31441eb01"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
