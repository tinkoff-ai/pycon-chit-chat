{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CAhkLnUKdTO"
      },
      "source": [
        "# Как устроены модели в хаггинфейсе на примере классификатора\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFWDyJHaKi7p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX1rRdTOLJTa"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGOYk4vHN88s",
        "outputId": "ea7f6583-3b78-4d49-a381-f62cf251e164"
      },
      "outputs": [],
      "source": [
        "text = 'i love you'\n",
        "model_input = tokenizer(\n",
        "    text, return_tensors='pt'\n",
        ")\n",
        "model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXnx8U4zLOop",
        "outputId": "c0083e5f-677d-401f-8caa-dadfc87490fa"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    probas = torch.softmax(model(**model_input).logits, dim=1)\n",
        "\n",
        "probas.numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixjoqTobOdqQ",
        "outputId": "f4be49f6-e801-4e77-b176-6cd470eb5ed3"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDvaxQrDJXhr"
      },
      "source": [
        "# Текстовые генеративные модели в хаггинфейсе\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wNRfEdfr_1nt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeprQjmZB9ct",
        "outputId": "2d001890-8233-43d5-c3da-4bef87a9b625"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "model = AutoModelWithLMHead.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3f4n5XROxJU",
        "outputId": "6f7bb4cb-65dd-4acc-f8f0-e9c9f84f59e6"
      },
      "outputs": [],
      "source": [
        "prompt = 'привет'\n",
        "model_input = tokenizer(prompt, return_tensors='pt')\n",
        "model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDt97KYUO3xy",
        "outputId": "885f56bb-fc39-4f50-9fb0-e40380b5c30c"
      },
      "outputs": [],
      "source": [
        "with torch.inference_mode():\n",
        "    model_output = model(**tokenizer(prompt, return_tensors='pt'))\n",
        "logits = model_output.logits\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL_ow4ffPaAJ",
        "outputId": "d0a03eed-4926-443a-efe5-5041fe94a089"
      },
      "outputs": [],
      "source": [
        "probas = logits[0, -1].softmax(dim=0)\n",
        "next_token_id = probas.argmax().item()\n",
        "next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1p2sLkf8P6B1",
        "outputId": "3d13a695-0cef-472b-9483-d8d717458226"
      },
      "outputs": [],
      "source": [
        "prompt += tokenizer.decode(16)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3XPOy7aTIGsm",
        "outputId": "10bdad96-4ab9-4c82-c0c4-da5115160e1d"
      },
      "outputs": [],
      "source": [
        "prompt = 'привет'\n",
        "max_new_tokens = 15\n",
        "for i in range(max_new_tokens):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**tokenizer(prompt, return_tensors='pt')).logits[0, -1]\n",
        "    probas = logits.softmax(dim=0)\n",
        "    next_token_id = probas.argmax().item()\n",
        "    prompt += tokenizer.decode(next_token_id)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2MXFqsxKkPS"
      },
      "source": [
        "https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin\n",
        "\n",
        "https://huggingface.co/docs/transformers/internal/generation_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwI0BAHhJED5",
        "outputId": "12c00f7d-2ae6-4313-92c0-da6208a16a1c"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer('привет', return_tensors='pt')\n",
        "generated_token_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=15\n",
        ")\n",
        "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
        "context_with_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-IU7KeWZZCr"
      },
      "source": [
        "# Как обучать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrvgBAtKZacE"
      },
      "source": [
        "https://huggingface.co/docs/transformers/tasks/language_modeling\n",
        "\n",
        "https://github.com/tinkoff-ai/pycon-chit-chat/blob/main/notebooks/lm_training.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi1MNK_aVSUU"
      },
      "source": [
        "# Методы декодирования"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCv94mQXV4R2",
        "outputId": "7ed9bda5-149e-4a6e-ed0c-581f9e8c69c0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
        "model = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
        "inputs = tokenizer('@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@', return_tensors='pt')\n",
        "generated_token_ids = model.generate(\n",
        "    **inputs,\n",
        "    top_k=10,  # sample one of k most likely\n",
        "    top_p=0.95,  # sample from those most likely which some >= p\n",
        "    num_beams=3,  # num beams for beam search\n",
        "    num_return_sequences=3,  # how many candidates to return\n",
        "    do_sample=True,  # do sample or greedy search\n",
        "    no_repeat_ngram_size=2,  # n grams of this n must not repeat in a text\n",
        "    temperature=1.0,  # make this value higher to get more interesting responses\n",
        "    repetition_penalty=1.2,  # make this value higher to fight with repetition \n",
        "    length_penalty=0.0001,  # < 1 for short texts, > 1 for long\n",
        "    eos_token_id=50257,  # when to stop\n",
        "    max_new_tokens=40  # how many tokens to generate\n",
        ")\n",
        "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
        "context_with_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjlm9Qs1Yd3N"
      },
      "source": [
        "# Crossencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIxQtxIoYhDl",
        "outputId": "023f0045-8653-46fb-d881-453cf468a70a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/response-quality-classifier-large')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('tinkoff-ai/response-quality-classifier-large')\n",
        "inputs = tokenizer('[CLS]привет[SEP]привет![SEP]как дела?[RESPONSE_TOKEN]норм, у тя как?', max_length=128, add_special_tokens=False, return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits\n",
        "    probas = torch.sigmoid(logits)[0].cpu().detach().numpy()\n",
        "relevance, specificity = probas\n",
        "relevance, specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I6_3WM1Ye5-"
      },
      "source": [
        "# Toxicity classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ48N1eGYtEC",
        "outputId": "fd4d547b-cb26-415c-818c-7fd8a8b491da"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/response-toxicity-classifier-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('tinkoff-ai/response-toxicity-classifier-base')\n",
        "inputs = tokenizer('[CLS]привет[SEP]привет![SEP]как дела?[RESPONSE_TOKEN]норм, у тя как?', max_length=128, add_special_tokens=False, return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits\n",
        "    probas = torch.sigmoid(logits)[0].cpu().detach().numpy()\n",
        "probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvAxuj_WLYCn",
        "outputId": "27a276e1-5c56-4461-b9b3-7fd3deffbf72"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer('[CLS]привет[SEP]привет![SEP]как дела?[RESPONSE_TOKEN]иди нахуй', max_length=128, add_special_tokens=False, return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits\n",
        "    probas = torch.softmax(logits, dim=1)[0].cpu().detach().numpy()\n",
        "probas.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjnbJfDZY5iX",
        "outputId": "a7e54acb-d59a-43f8-d740-ed141eb5fafd"
      },
      "outputs": [],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "huggingface usage.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': poetry)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ef03fb1679aeac65da8746bbb9907ec69320f4b4b60f937c1b2a3f31441eb01"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
