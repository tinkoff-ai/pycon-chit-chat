{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "leyX9GTLlnBO"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n",
        "# with open('/content/drive/My Drive/foo.txt', 'w') as f:\n",
        "#   f.write('Hello Google Drive!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CAhkLnUKdTO"
      },
      "source": [
        "# Как устроены модели в хаггинфейсе на примере классификатора\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "OFWDyJHaKi7p"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "NX1rRdTOLJTa"
      },
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGOYk4vHN88s",
        "outputId": "ea7f6583-3b78-4d49-a381-f62cf251e164"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 101, 1045, 2293, 2017,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1]])}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text = 'i love you'\n",
        "model_input = tokenizer(\n",
        "    text, return_tensors='pt'\n",
        ")\n",
        "model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXnx8U4zLOop",
        "outputId": "c0083e5f-677d-401f-8caa-dadfc87490fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0.0001343627372989431, 0.9998656511306763]]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "    probas = torch.softmax(model(**model_input).logits, dim=1)\n",
        "\n",
        "probas.numpy().tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixjoqTobOdqQ",
        "outputId": "f4be49f6-e801-4e77-b176-6cd470eb5ed3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'NEGATIVE', 1: 'POSITIVE'}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDvaxQrDJXhr"
      },
      "source": [
        "# Текстовые генеративные модели в хаггинфейсе\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wNRfEdfr_1nt"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelWithLMHead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeprQjmZB9ct",
        "outputId": "2d001890-8233-43d5-c3da-4bef87a9b625"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')\n",
        "model = AutoModelWithLMHead.from_pretrained('sberbank-ai/rugpt3small_based_on_gpt2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3f4n5XROxJU",
        "outputId": "6f7bb4cb-65dd-4acc-f8f0-e9c9f84f59e6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[960, 577]]), 'attention_mask': tensor([[1, 1]])}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = 'привет'\n",
        "model_input = tokenizer(prompt, return_tensors='pt')\n",
        "model_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDt97KYUO3xy",
        "outputId": "885f56bb-fc39-4f50-9fb0-e40380b5c30c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[-7.9739, -9.1519, -8.5353,  ..., -8.2743, -8.5468, -8.3682],\n",
              "         [-8.0564, -8.2150, -7.3547,  ..., -7.7663, -7.8596, -8.0142]]])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with torch.inference_mode():\n",
        "    model_output = model(**tokenizer(prompt, return_tensors='pt'))\n",
        "logits = model_output.logits\n",
        "logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LL_ow4ffPaAJ",
        "outputId": "d0a03eed-4926-443a-efe5-5041fe94a089"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "probas = logits[0, -1].softmax(dim=0)\n",
        "next_token_id = probas.argmax().item()\n",
        "next_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "1p2sLkf8P6B1",
        "outputId": "3d13a695-0cef-472b-9483-d8d717458226"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'привет,'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt += tokenizer.decode(16)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3XPOy7aTIGsm",
        "outputId": "10bdad96-4ab9-4c82-c0c4-da5115160e1d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'привет, я тут, в Москве, в гостях у одного человека, который мне'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "prompt = 'привет'\n",
        "max_new_tokens = 15\n",
        "for i in range(max_new_tokens):\n",
        "    inputs = tokenizer(prompt, return_tensors='pt')\n",
        "    with torch.inference_mode():\n",
        "        logits = model(**tokenizer(prompt, return_tensors='pt')).logits[0, -1]\n",
        "    probas = logits.softmax(dim=0)\n",
        "    next_token_id = probas.argmax().item()\n",
        "    prompt += tokenizer.decode(next_token_id)\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2MXFqsxKkPS"
      },
      "source": [
        "https://huggingface.co/docs/transformers/v4.20.1/en/main_classes/text_generation#transformers.generation_utils.GenerationMixin\n",
        "\n",
        "https://huggingface.co/docs/transformers/internal/generation_utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwI0BAHhJED5",
        "outputId": "12c00f7d-2ae6-4313-92c0-da6208a16a1c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['привет, я тут, в Москве, в гостях у одного человека, который мне']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer('привет', return_tensors='pt')\n",
        "generated_token_ids = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=15\n",
        ")\n",
        "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
        "context_with_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-IU7KeWZZCr"
      },
      "source": [
        "# Как обучать"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SrvgBAtKZacE"
      },
      "source": [
        "https://huggingface.co/docs/transformers/tasks/language_modeling\n",
        "\n",
        "https://github.com/tinkoff-ai/pycon-chit-chat/blob/main/notebooks/lm_training.ipynb\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oi1MNK_aVSUU"
      },
      "source": [
        "# Методы декодирования"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCv94mQXV4R2",
        "outputId": "7ed9bda5-149e-4a6e-ed0c-581f9e8c69c0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:973: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Setting `pad_token_id` to `eos_token_id`:50257 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@нормально,а у тебя?как день прошел?что делаешь?у меня тоже день не задался,но я исправлюсь 😊😉🤗👍🏻',\n",
              " '@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@нормально, а у тебя? Как сам? Я вот на работу собираюсь, так что не скучай! 👋🏻😉😁😊😂😘�',\n",
              " '@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@нормально, а у тебя как? Как сам? Что нового? А то я волнуюсь за тебя. 🙃👍🏻🤣💪�']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
        "model = AutoModelWithLMHead.from_pretrained('tinkoff-ai/ruDialoGPT-medium')\n",
        "inputs = tokenizer('@@ПЕРВЫЙ@@ привет @@ВТОРОЙ@@ привет @@ПЕРВЫЙ@@ как дела? @@ВТОРОЙ@@', return_tensors='pt')\n",
        "generated_token_ids = model.generate(\n",
        "    **inputs,\n",
        "    top_k=10,  # sample one of k most likely\n",
        "    top_p=0.95,  # sample from those most likely which some >= p\n",
        "    num_beams=3,  # num beams for beam search\n",
        "    num_return_sequences=3,  # how many candidates to return\n",
        "    do_sample=True,  # do sample or greedy search\n",
        "    no_repeat_ngram_size=2,  # n grams of this n must not repeat in a text\n",
        "    temperature=1.0,  # make this value higher to get more interesting responses\n",
        "    repetition_penalty=1.2,  # make this value higher to fight with repetition \n",
        "    length_penalty=0.0001,  # < 1 for short texts, > 1 for long\n",
        "    eos_token_id=50257,  # when to stop\n",
        "    max_new_tokens=40  # how many tokens to generate\n",
        ")\n",
        "context_with_response = [tokenizer.decode(sample_token_ids) for sample_token_ids in generated_token_ids]\n",
        "context_with_response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjlm9Qs1Yd3N"
      },
      "source": [
        "# Crossencoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fIxQtxIoYhDl",
        "outputId": "023f0045-8653-46fb-d881-453cf468a70a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(0.79521185, 0.4289922)"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/response-quality-classifier-large')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('tinkoff-ai/response-quality-classifier-large')\n",
        "inputs = tokenizer('[CLS]привет[SEP]привет![SEP]как дела?[RESPONSE_TOKEN]норм, у тя как?', max_length=128, add_special_tokens=False, return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits\n",
        "    probas = torch.sigmoid(logits)[0].cpu().detach().numpy()\n",
        "relevance, specificity = probas\n",
        "relevance, specificity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7I6_3WM1Ye5-"
      },
      "source": [
        "# Toxicity classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJ48N1eGYtEC",
        "outputId": "fd4d547b-cb26-415c-818c-7fd8a8b491da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([0.9970234 , 0.15969415, 0.07131252, 0.3618246 ], dtype=float32)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "tokenizer = AutoTokenizer.from_pretrained('tinkoff-ai/response-toxicity-classifier-base')\n",
        "model = AutoModelForSequenceClassification.from_pretrained('tinkoff-ai/response-toxicity-classifier-base')\n",
        "inputs = tokenizer('[CLS]привет[SEP]привет![SEP]как дела?[RESPONSE_TOKEN]норм, у тя как?', max_length=128, add_special_tokens=False, return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits\n",
        "    probas = torch.sigmoid(logits)[0].cpu().detach().numpy()\n",
        "probas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvAxuj_WLYCn",
        "outputId": "27a276e1-5c56-4461-b9b3-7fd3deffbf72"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0.0009251515730284154,\n",
              " 0.0036098435521125793,\n",
              " 0.9868594408035278,\n",
              " 0.008605570532381535]"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "inputs = tokenizer('[CLS]привет[SEP]привет![SEP]как дела?[RESPONSE_TOKEN]иди нахуй', max_length=128, add_special_tokens=False, return_tensors='pt')\n",
        "with torch.inference_mode():\n",
        "    logits = model(**inputs).logits\n",
        "    probas = torch.softmax(logits, dim=1)[0].cpu().detach().numpy()\n",
        "probas.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjnbJfDZY5iX",
        "outputId": "a7e54acb-d59a-43f8-d740-ed141eb5fafd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 'ok', 1: 'risks', 2: 'severe_toxic', 3: 'toxic'}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.config.id2label"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "huggingface usage.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
